\documentclass{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{txfonts}
\usepackage{fancyhdr}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\graphicspath{ {images/} }

\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}

\begin{document}
\fancyhead[R]{\null\hfill\begin{tabular}[t]{l@{}}
	\textbf{Daniel Hartig} \\
	Stat 652 \\
	Homework 5
\end{tabular}}
\setlength{\headheight}{4\baselineskip}



\subsection*{Problem 1}
\subsubsection*{a.}
\begin{equation*}
\begin{aligned}
P(X_i < x) &= F(x) = \int e^{\theta-x}I_{(\theta,\infty)}(x)dx = 
\begin{cases}1-e^{\theta-x} & x\geq\theta \\ 0 & x < \theta\end{cases} \\
P(X_i > x) &= 1 - F(x) = 
\begin{cases}e^{\theta-x} & x\geq\theta \\ 0 & x < \theta\end{cases} \\
P(X_{(1)} > x) &= \sum_{i=1}^n P(X_i > x) = \left[P(X_i > x)\right]^n =
\begin{cases}e^{n(\theta-x)} & x\geq\theta \\ 0 & x < \theta\end{cases} \\
P(X_{(1)} < x) &= F_{X_{(1)}}(x) = 
\begin{cases}1-e^{n(\theta-x)} & x\geq\theta \\ 0 & x < \theta\end{cases} \\
f_{X_{(1)}}(x) &= ne^{n(\theta-x)}I_{(\theta,\infty)}(x)
\end{aligned}
\qquad\qquad
\begin{aligned}
E(X_{(1)}) &= \int_\theta^\infty xne^{n(\theta-x)}dx = ne^{n\theta}\int_\theta^\infty xe^{-nx}dx \\
&=ne^{n\theta}\left.\left[\frac{-(nx+1)e^{-nx}}{n^2}\right]\right|^\infty_\theta \\
&=ne^{n\theta}\frac{(n\theta+1)e^{-n\theta}}{n^2} \\
&=\frac{n\theta+1}{n} = \theta + \frac{1}{n}
\end{aligned}
\end{equation*}

\subsubsection*{b.}
\begin{equation*}
\begin{aligned}
E\left(X_{(1)}^2\right) &= ne^{n\theta}\int_\theta^\infty x^2e^{-nx}dx \\
&=ne^{n\theta}\left.\left[\frac{n^2x^2+2nx+2}{n^3}e^{-nx}\right]\right|^\infty_\theta
&= \theta^2+\frac{2}{n}\theta+\frac{2}{n^2}
\end{aligned}
\qquad\qquad
\begin{aligned}
E\left[(X_{(1)} - \theta)^2\right] &= E\left[X_{(1)}^2 - 2\theta X_{(1))} + \theta^2\right] \\
&= E\left(X_{(1)}^2\right) - 2\theta E(X_{(1))})+\theta^2 \\
&=\theta^2+\frac{2}{n}\theta+\frac{2}{n^2} - 2\theta\left(\theta + \frac{1}{n}\right) + \theta^2 \\
&= \frac{2}{n^2}
\end{aligned}
\end{equation*}

\subsubsection*{c.}
\[\hat{\theta}_{MLE} = min\left(X_1, ..., X_n\right)\]
To choose between the location-equivariant estimator and the scale-equivariant estimator, we assess both
\begin{align*}
\hat{\theta}_{MLE}(X_1+c, ...,X_n+c) &= min(X_1+c, ...,X_n+c) \\
&=min(X_1, ..., X_n) + c \\ \\
\hat{\theta}_{MLE}(cX_1, ..., cX_n) & = min(cX_1, ..., cX_n) \\
&=c\cdot min(X_1, ..., X_n)
\end{align*}
Either the location-equivariant or scale-equivariant version of the Pittman estimator is valid. We choose the location equivariant for simplicity of calculation. The likelihood function is 
\[L(\theta|\mathbf{x}) = \exp\left(\sum_{i=1}^n \theta - x_i\right)\]
The range of the parameter $\theta$ is $(-\infty, infty)$. However, since we also have the constraint $x\geq\theta$, $\theta$ must be smaller than the smallest $x_i$ in our sample. Therefore we set the bounds of $\theta$ at $(-\infty, X_{(1)})$.
\pagebreak
\begin{equation*}
\begin{aligned}
\int_{-\infty}^{x_{(1)}}\theta\exp\left(\sum_{i=1}^n \theta - x_i\right)d\theta &=\exp\left(-\sum_{i=1}^n x_i\right)\int_{-\infty}^{x_{(1)}}\theta e^{n\theta}d\theta \\
&=\exp\left(-\sum_{i=1}^n x_i\right)\left.\left[\frac{n\theta-1}{n^2}e^{n\theta}\right]\right|^{x_{(1)}}_{-\infty} \\
&=\exp\left(-\sum_{i=1}^n x_i\right)\frac{nx_{(1)}-1}{n^2}\exp\left(nx_{(1)}\right) \\
&=\frac{nx_{(1)}-1}{n^2}\exp\left(-\sum_{i=1}^n x_{(1)} -x_i\right)
\end{aligned}
\qquad\qquad
\begin{aligned}
\int_{-\infty}^{x_{(1)}}\exp\left(\sum_{i=1}^n \theta - x_i\right)d\theta &=\exp\left(-\sum_{i=1}^n x_i\right)\int_{-\infty}^{x_{(1)}}e^{n\theta}d\theta \\ 
&=\exp\left(-\sum_{i=1}^n x_i\right)\left.\left[\frac{e^{n\theta}}{n}\right]\right|^{x_{(1)}}_{-\infty} \\
&=\exp\left(-\sum_{i=1}^n x_i\right)\frac{1}{n}\exp\left(nx_{(1)}\right) \\
&=\frac{1}{n}\exp\left(-\sum_{i=1}^n x_{(1)} -x_i\right)
\end{aligned}
\end{equation*}
\begin{align*}
\hat{\theta}_{PL} &= \frac{\int\theta L(\theta|\mathbf{x}) d\theta}{\int L(\theta|\mathbf{x}) d\theta} \\
&=\frac{\frac{nx_{(1)}-1}{n^2}\exp\left(-\sum_{i=1}^n x_{(1)} -x_i\right)}{\frac{1}{n}\exp\left(-\sum_{i=1}^n x_{(1)} -x_i\right)} \\
&= \frac{nx_{(1)}-1}{n} = x_{(1)} - \frac{1}{n}
\end{align*}
\subsubsection*{d.}
\begin{align*}
E\left[(X_{(1)} -\frac{1}{n} - \theta)^2\right] &= E\left[X_{(1)}^2 + \frac{1}{n^2} + \theta^2 - 2\theta X_{(1))} - \frac{2}{n} X_{(1))} + \frac{2\theta}{n}\right] \\
&= E\left(X_{(1)}^2\right) + \frac{1}{n^2} + \theta^2 - 2\theta E(X_{(1))}) - \frac{2}{n} E(X_{(1))}) + \frac{2\theta}{n} \\
&=\theta^2+\frac{2\theta}{n}+\frac{2}{n^2} + \frac{1}{n^2} + \theta^2 - 2\theta\left(\theta + \frac{1}{n}\right) -\frac{2}{n}\left(\theta + \frac{1}{n}\right) + \frac{2\theta}{n} \\
&= \frac{1}{n^2}
\end{align*}


\subsection*{Problem 2}
\subsubsection*{a.}
\[\pi(\theta) = I_{(0,1)}(\theta)\]
For the following equations, we will represent the summation over the set $i = 1, 2, ..., n$ with just the $\sum$ symbol. 
\begin{align*}
f(\mathbf{x}|\theta) &= \theta^n(1-\theta)^{-n+\sum x_i} \\ \\
f(\mathbf{x}|\theta)\pi(\theta) &= \theta^n(1-\theta)^{-n+\sum x_i}I_{(0,1)}(\theta) \\
\int_0^1 f(\mathbf{x}|\theta)\pi(\theta) d\theta &= \int_0^1 \theta^n(1-\theta)^{-n+\sum x_i} d\theta \\
&=\frac{\Gamma(n+1)\Gamma(1-n+\sum x_i)}{\Gamma(2+ \sum x_i)}\int_0^1\frac{\Gamma(2+ \sum x_i)}{\Gamma(n+1)\Gamma(1-n+\sum x_i)}\theta^n(1-\theta)^{-n+\sum x_i} d\theta
\end{align*}
\pagebreak

The portion inside the integral is the density function of beta distribution with parameters $\alpha = n+1$ and $\beta = 1 - n + \sum x_i$. The beta distribution integrates to 1 over the range $(0, 1)$. 
\begin{align*}
\pi(\theta|\mathbf{x}) &= \frac{f(\mathbf{x}|\theta)\pi(\theta)}{\int_0^1 f(\mathbf{x}|\theta)\pi(\theta) d\theta} \\
&=\frac{\theta^n(1-\theta)^{-n+\sum x_i}}{\frac{\Gamma(n+1)\Gamma(1-n+\sum x_i)}{\Gamma(2+ \sum x_i)}} \\
&=\frac{\Gamma(2+ \sum x_i)}{\Gamma(n+1)\Gamma(1-n+\sum x_i)}\theta^n(1-\theta)^{-n+\sum x_i}
\end{align*}
The density function of the posterior distribution is itself the same beta distribution from above. 

\subsubsection*{c.}

Following the derivation of the Bayes minimizer based on the squared error loss function from page 7.3.36 of the notes, we see that $\hat{\theta}(\mathbf{x}) = E(\theta|\mathbf{x})$.
Since $\pi(\theta|\mathbf{x})$ has as gamma distribution, 
\begin{align*}
\hat{\theta}(\mathbf{x}) = E(\theta|\mathbf{x}) &= \frac{\alpha}{\alpha + \beta} \\
&= \frac{n+1}{2+\sum_{i=1}^n x_i}
\end{align*}
\subsection*{Problem 3}
For this problem, there is a single sample observation which we will denote by $x$.
\begin{align*}
\pi(\theta) &= \frac{1}{\sqrt{2\pi}}\exp\left(\frac{-\theta^2}{2}\right) \\ \\ 
f(\mathbf{x}|\theta) &= \frac{1}{\sqrt{2\pi}}\exp\left(\frac{-[x-\theta]^2}{2}\right) \\ \\ 
f(\mathbf{x}|\theta)\pi(\theta) &= \frac{1}{2\pi}\exp\left(\frac{-x^2}{2}+x\theta+\theta^2\right) \\ \\
\int_{-\infty}^\infty f(\mathbf{x}|\theta)\pi(\theta) d\theta &= \int_{-\infty}^\infty\frac{1}{2\pi}\exp\left(\frac{-x^2}{2}+x\theta-\theta^2\right)d\theta \\
&=\frac{1}{2\sqrt{\pi}}\exp\left(\frac{-x^2}{4}\right)\int_{-\infty}^\infty \frac{1}{\sqrt{\pi}}\exp\left(\frac{-x^2}{4}+x\theta-\theta^2\right)d\theta \\ 
&=\frac{1}{2\sqrt{\pi}}\exp\left(\frac{-x^2}{4}\right)\int_{-\infty}^\infty \frac{1}{\sqrt{\pi}}\exp\left(-\left[[\theta-\frac{x}{2}\right]^2\right) d\theta
\end{align*}
The portion inside the integral is the density function of a normal distribution with variable $\theta$, $\mu = x/2$, $\sigma^2 = 1/2$. Over the range $(-\infty, \infty)$, the normal distribution integrates to 1. 
\begin{align*}
\int_{-\infty}^\infty f(\mathbf{x}|\theta)\pi(\theta) d\theta &= \frac{1}{2\sqrt{\pi}}\exp\left(\frac{-x^2}{4}\right) \\ \\
\pi(\theta|\mathbf{x}) &= \frac{f(\mathbf{x}|\theta)\pi(\theta)}{\int_{-\infty}^\infty f(\mathbf{x}|\theta)\pi(\theta) d\theta} \\
&= \frac{\frac{1}{2\pi}\exp\left(\frac{-x^2}{2}+x\theta+\theta^2\right)}{\frac{1}{2\sqrt{\pi}}\exp\left(\frac{-x^2}{4}\right)} \\
&=\frac{1}{\sqrt{\pi}}\exp\left(-\left[[\theta-\frac{x}{2}\right]^2\right)
\end{align*}
The density function of the posterior distribution is itself the same normal distribution from above. As shown in the assignment, the posterior expected loss is given by 
\[\int e^{(\hat{\theta}-\theta)/2}\pi(\theta|\mathbf{x})d\theta - \frac{1}{2}\hat{\theta}\int \pi(\theta|\mathbf{x})d\theta + \frac{1}{2}\int\theta\pi(\theta|\mathbf{x})d\theta- \int\pi(\theta|\mathbf{x})d\theta
\]
with all integrals over the range of $\theta\in(-\infty, \infty)$. 
\begin{align*}
\int_{-\infty}^\infty e^{(\hat{\theta}-\theta)/2}\pi(\theta|\mathbf{x})d\theta &= \int_{-\infty}^\infty e^{(\hat{\theta}-\theta)/2} \frac{1}{\sqrt{\pi}}\exp\left(-\left[[\theta-\frac{x}{2}\right]^2\right) d\theta \\
&= \frac{1}{\sqrt{\pi}}\exp\left(\frac{\hat{\theta}}{2}\right)\int_{-\infty}^\infty \exp\left(-\frac{\theta}{2}\right)\exp\left(-\theta^2+x\theta-\frac{x^2}{4}\right)d\theta \\
&= \frac{1}{\sqrt{\pi}}\exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) \int_{-\infty}^\infty \exp\left(-\left[\theta^2-\theta\left(x-\frac{1}{2}\right) + \frac{x^2 - x + 1/4}{4}\right]\right) \\
&= \exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) \int_{-\infty}^\infty \frac{1}{\sqrt{\pi}}\exp\left(-\left[\theta - \left(\frac{x}{2}-\frac{1}{4}\right)\right]^2\right)
\end{align*}
The portion inside the integral is the density function of a normal distribution with variable $\theta$, $\mu = x/2-1/4$, $\sigma^2 = 1/2$. Over the range $(-\infty, \infty)$, the normal distribution integrates to 1. The posterior distribution is a valid pdf, therefore over the range $(-\infty, \infty)$ it integrates to 1. The integral of $\theta\pi(\theta|\mathbf{x})$ over $\theta\in(-\infty, \infty)$ is $E(\theta|\mathbf{x})$. 
\begin{align*}
\int e^{(\hat{\theta}-\theta)/2}\pi(\theta|\mathbf{x})d\theta - \frac{1}{2}\hat{\theta}\int \pi(\theta|\mathbf{x})d\theta + \frac{1}{2}\int\theta\pi(\theta|\mathbf{x})d\theta- \int\pi(\theta|\mathbf{x})d\theta &= \exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) - \frac{1}{2}\hat{\theta} + \frac{1}{2}E(\theta|\mathbf{x}) - 1
\end{align*}
To minimize the posterior expected loss function, we take its derivative with respect to $\hat{\theta}(\mathbf{x})$,
\[\frac{d}{d\hat{\theta}}\left[\exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) - \frac{1}{2}\hat{\theta} + \frac{1}{2}E(\theta|\mathbf{x}) - 1 \right] = \frac{1}{2}\exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) - \frac{1}{2}\]
and set the resulting expression equal to zero: 
\begin{align*}
\frac{1}{2}\exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) - \frac{1}{2} &= 0 \\
\exp\left(\frac{\hat{\theta}}{2}\right)\exp\left(-\frac{x}{4}+\frac{1}{16}\right) &= 1 \\ 
\exp\left(\frac{\hat{\theta}}{2}\right) &= \exp\left(\frac{x}{4}-\frac{1}{16}\right) \\
\frac{\hat{\theta}}{2} &= \frac{x}{4}-\frac{1}{16} \\
\hat{\theta} &= \frac{x}{2}-\frac{1}{8}
\end{align*}
\pagebreak

All functions of the form $\exp(a\hat{\theta})$ are increasing on $(-\infty, \infty)$ if $a$ is positive. Since the derivative of the posterior loss function is zero at the point above, and it is increasing, then the derivative of the posterior loss function must be negative for $\hat{\theta} < x/2-1/8$ and positive for $\hat{\theta} > x/2-1/8$. Therefore, this point is the global minimum, and the Bayes estimate for $\hat{\theta}$. 

\subsection*{Problem 4}
\subsubsection*{a. }
The UMVUE for $\theta$ from a series of Bernoulli($\theta$) distributions is 
\[\hat{\theta}_U = \frac{T(\mathbf{X})}{n}\] as demonstrated in HW4, Problem 2. The sufficient statistic $T(\mathbf{X})$ itself has a binomial distribution, so 
\[E\left(T(\mathbf{X})\right) = n\theta;\qquad \mathrm{Var}\left(T(\mathbf{X})\right) = n\theta(1-\theta);\qquad E\left(T(\mathbf{X})^2\right) = \mathrm{Var}\left(T(\mathbf{X})\right) + \left[E\left(T(\mathbf{X})\right)\right]^2 = n\theta - n\theta^2 + n^2\theta^2 = n\theta + n(n-1)\theta^2\]
 The MSE for Mr. B's estimate of $\hat{\theta}_B = 9/19$ is 
\begin{align*}
MSE_{B} = \sum_{i=1}^n\left(\frac{10}{19} - \frac{9}{19}\right)^2 = \frac{n}{19^2}.
\end{align*}
The MSE for $\hat{\theta}_U = T(\mathbf{X})/n$ is 
\begin{align*}
MSE_U = E\left(\left[\hat{\theta}_U - \theta\right]^2\right) &= \frac{1}{n^2}E\left(T(\mathbf{X})^2\right) - \frac{2\theta}{n} E\left(T(\mathbf{X})\right) + \theta^2 \\
&=\frac{1}{n^2}n\theta + \frac{1}{n^2}n(n-1)\theta^2 -  \frac{2\theta}{n}n\theta + \theta^2 \\
&= \frac{1}{n}\theta + \left(\frac{n-1}{n} - 2 + 1\right)\theta^2
= \frac{\theta(1-\theta)}{n}
\end{align*}
Plugging that actual value of $\theta=10/19$, we get 
\[MSE_U = \frac{1}{n}\frac{10}{19}\frac{9}{19} = \frac{90}{n19^2}\]
To solve for $MSE_U < MSE_B$, 
\begin{align*}
MSE_U &< MSE_B \\
\frac{90}{n19^2} &< frac{n}{19^2} \\
90 &< n^2 
\end{align*}
Therefore, the sample size must be $n\geq 10$ to fulfull the inequality above and ensure that Mr. U's unbiased estimator has a smaller MSE than Mr. B's. 

\subsubsection*{b.}
The distribution of the sufficient statistic $T(\mathbf{X})$ is binomial, and will thus yield a integer. Given a sample size of ten, the only integer which, when divided by 10, give a closer estimate than $9/19 = 0.4736$ is $5/10 = 0.5$. The distance from $6/10 = 0.6$ to $10/19 = 0.526$ is greater than that between $9/19$ and $10/19$. For a binomial random variable with sample size 10 and parameter $\theta = 10/19 = 0.526$, the probability that there will be exactly $5$ successes is 
\[P\left(T(\mathbf{X}) = 5\right) = \binom{10}{5}\left(\frac{10}{19}\right)^5\left(\frac{9}{19}\right)^5 = 0.243\]

\end{document}